{
    "BOS_IDX": 0,
    "EOS_IDX": 1,
    "PAD_IDX": 2,
    "special_tokens": ["<bos>", "<eos>", "<pad>"],
    "vocab_size": 20000,
    "max_length": 100,
    "tokens_per_batch": 100,
    "epochs": 1,
    "grad_accumulation": 1,
    "d_model": 1,
    "n_heads": 1,
    "d_ff": 1,
    "n_layers": 1,
    "dropout": 0.0
}