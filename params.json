{
    "BOS_IDX": 0,
    "EOS_IDX": 1,
    "PAD_IDX": 2,
    "special_tokens": ["<bos>", "<eos>", "<pad>"],
    "vocab_size": 20000,
    "max_length": 100,
    "tokens_per_batch": 5000,
    "epochs": 20,
    "grad_accumulation": 16,
    "d_model": 512,
    "n_heads": 8,
    "d_ff": 1024,
    "n_layers": 6,
    "dropout": 0.1
}