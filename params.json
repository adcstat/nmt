{
    "BOS_IDX": 0,
    "EOS_IDX": 1,
    "PAD_IDX": 2,
    "special_tokens": ["<bos>", "<eos>", "<pad>"],
    "vocab_size": 32000,
    "max_length": 100,
    "tokens_per_batch": 2500,
    "periods": 8,
    "epochs_till_restart": 3,
    "grad_accumulation": 40,
    "d_model": 1024,
    "n_heads": 16,
    "d_ff": 3072,
    "n_layers": 6,
    "dropout": 0.3
}