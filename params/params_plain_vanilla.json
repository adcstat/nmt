{
    "vocab_size": 32000,
    "max_length": 200,
    "tokens_per_batch": 3125,
    "epochs": 25,
    "grad_accumulation": 2,
    "d_model": 512,
    "n_heads": 8,
    "d_ff": 2048,
    "n_layers": 6,
    "dropout": 0.1,
    "warmup_steps": 4000,
    "max_lr": 0.0006987712429686843
}