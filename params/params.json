{
    "vocab_size": 32000,
    "max_length": 200,
    "tokens_per_batch": 4000,
    "epochs": 25,
    "grad_accumulation": 12,
    "d_model": 512,
    "n_heads": 8,
    "d_ff": 2048,
    "n_layers": 6,
    "dropout": 0.1
}